{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a786e360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aca326e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf90ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87498fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c916b",
   "metadata": {},
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5254c288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How would you approach designing a fair and effective language model evaluation metric that accounts for nuance and bias in both the training data and the model's outputs?\n"
     ]
    }
   ],
   "source": [
    "from openai import responses\n",
    "\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4c01cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51cb19",
   "metadata": {},
   "source": [
    "### O4-Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9e67801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Designing a fair and effective evaluation metric for language models that accounts for nuance and bias involves several steps, and it typically spans qualitative and quantitative approaches. Here’s a framework to develop such a metric:\n",
      "\n",
      "### 1. Define Evaluation Objectives\n",
      "\n",
      "**a. Purpose of Evaluation:**  \n",
      "Understand what you want to evaluate. Is it coherence, relevance, factual correctness, or bias and fairness? Different goals may require different evaluation strategies.\n",
      "\n",
      "**b. Stakeholder Perspectives:**  \n",
      "Consider the perspectives of various stakeholders (users, developers, ethicists). This may include cultural interpretations, impact on marginalized groups, or specific use cases.\n",
      "\n",
      "### 2. Establish a Baseline Metric Suite\n",
      "\n",
      "**a. Quantitative Metrics:**  \n",
      "- **Perplexity:** Traditional measure but can be biased towards language frequency.\n",
      "- **BLEU, ROUGE, METEOR:** Useful for specific tasks like translation or summarization, but can miss nuances.\n",
      "  \n",
      "**b. Qualitative Metrics:**  \n",
      "- **Human Judgments:** Utilize crowdsourced evaluations to assess outputs based on clarity, coherence, and relevance.\n",
      "- **Expert Evaluations:** Involve domain experts for specific applications where nuance matters (e.g., social sciences).\n",
      "\n",
      "### 3. Assess Bias\n",
      "\n",
      "**a. Training Data Analysis:**  \n",
      "- **Data Auditing:** Analyze the training data for demographic representation and potential biases. Employ tools to measure the presence of stereotypes or discriminatory language.\n",
      "  \n",
      "**b. Output Bias Evaluation:**  \n",
      "- **Bias Benchmarking:** Develop benchmarks that specifically measure bias in outputs (e.g., gender bias tests, racial bias metrics).\n",
      "- **Adversarial Testing:** Create adversarial examples that expose biased behavior or problematic outputs in various contexts.\n",
      "\n",
      "### 4. Incorporate Nuance\n",
      "\n",
      "**a. Contextual Understanding:**  \n",
      "Integrate multi-turn dialogues or prompts that can capture subtleties in meaning. Use context-aware assessments to gauge how well the model captures nuances in different scenarios.\n",
      "\n",
      "**b. Scenario-based Evaluation:**  \n",
      "Develop a diverse range of scenarios that require understanding of social, historical, or emotional contexts, and evaluate model outputs on how well they handle these situations.\n",
      "\n",
      "### 5. Hybrid Approaches\n",
      "\n",
      "**a. Weighted Metrics:**  \n",
      "Create a composite metric that weights significance based on context. For instance, assign higher importance to fairness and nuance for specific applications (like healthcare) while allowing for more lenient evaluation in other domains.\n",
      "\n",
      "**b. Continuous Feedback Loop:**  \n",
      "Implement a system for ongoing evaluation and feedback. Adjust the evaluation process based on emergent trends, cultural changes, or user feedback to ensure ongoing relevance and fairness.\n",
      "\n",
      "### 6. Ethical Considerations\n",
      "\n",
      "**a. Transparency:**  \n",
      "Document evaluation processes and the rationale behind selected metrics, enabling accountability.\n",
      "\n",
      "**b. Inclusive Design:**  \n",
      "Involve diverse teams in metric design to incorporate multiple perspectives, especially from marginalized communities likely to be impacted by biases in AI systems.\n",
      "\n",
      "### 7. Iterate and Evolve\n",
      "\n",
      "**a. Pilot Studies:**  \n",
      "Conduct pilot evaluations with various datasets and tasks to refine metrics and evaluation approaches.\n",
      "\n",
      "**b. Community Engagement:**  \n",
      "Engage with the research community and stakeholders to keep abreast of developments in fairness metrics and bias detection methodologies. \n",
      "\n",
      "By employing this multi-faceted approach, the proposed evaluation metric can effectively gauge the performance of language models while accounting for the complexities of bias and nuance. Regular updates and community collaboration are key to adapting and improving the metric’s effectiveness over time.\n"
     ]
    }
   ],
   "source": [
    "open_ai_response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": question}],\n",
    ")\n",
    "\n",
    "print(open_ai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9819b8",
   "metadata": {},
   "source": [
    "### OLLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7e932",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m ollama_model_name = \u001b[33m\"\u001b[39m\u001b[33mdeepseek-r1:latest\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m response = ollama_deepseek_r1.chat.completions.create(model=ollama_model_name, messages=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: question}]),\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m answer = \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoices\u001b[49m[\u001b[32m0\u001b[39m].message.content\n\u001b[32m      7\u001b[39m display(Markdown(answer))\n",
      "\u001b[31mAttributeError\u001b[39m: 'tuple' object has no attribute 'choices'"
     ]
    }
   ],
   "source": [
    "ollama_deepseek_r1 = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "ollama_model_name = \"deepseek-r1:latest\"\n",
    "\n",
    "response = ollama_deepseek_r1.chat.completions.create(model=ollama_model_name, messages=[{\"role\": \"user\", \"content\": question}]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76c6fb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-824', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='<think>\\nFirst, the user is asking about designing a language model evaluation metric. The key points are: it should be fair, effective, account for nuance, and handle bias from both the training data and the model\\'s outputs.\\n\\n**Understanding Fairness:** Fairness in evaluation means that the metric shouldn\\'t disadvantage certain groups or behaviors based on sensitive attributes like race, gender, religion, etc. I need to ensure that biases aren\\'t amplified through the metric itself; it should assess performance without unfairly penalizing specific aspects.\\n\\n**Effectiveness:** This refers to how well the metric captures true model performance. It should be reliable and provide meaningful insights into quality, such as coherence, relevance, and informativeness.\\n\\n**Accounting for Nuance:** Language models often deal with subtle meanings, context, sarcasm, irony, etc. A good metric shouldn\\'t just look at surface-level metrics but should incorporate depth to evaluate how well the model handles these complexities.\\n\\n**Bias from Training Data and Model Outputs:** Bias can come from two sources: training data (which might contain stereotypes or unfair information) and model outputs (where these biases manifest). The metric needs to detect if the bias is present, measure its severity, and ideally provide guidelines to mitigate it without compromising quality.\\n\\nI should think about existing metrics that address some of this. Common ones include BLEU for translation, ROUGE for summarization, F1 score for classification. But none fully handle nuance and bias in a holistic way.\\n\\nFor language models like GPT-3 or Mistral, evaluation is often done based on human preferences, but that\\'s subjective and expensive. We need something quantitative.\\n\\n**Proposed Approach:** I\\'ll outline a multi-faceted metric that combines automated elements with fairness considerations. Since it accounts for nuance (which might require some semantic understanding), perhaps use techniques from natural language processing or machine learning that go beyond bag-of-words, like contextual embeddings or causal inference to detect biases.\\n\\nHere\\'s a breakdown of how I might structure the evaluation:\\n\\n1. **Lexical Diversity and Fluency:** Check if responses are grammatically correct, diverse in word choice, etc., but this is basic.\\n\\n2. **Coherence and Consistency:** Ensure that the model generates consistent and logical outputs. But handling nuance could involve assessing similarity to a context or previous parts of the response.\\n\\nFor bias detection, there might be specific datasets like those for toxicity or stereotypes from Fairness in Machine Learning (FML).\\n\\n**Fairness Metrics:** There are fairness metrics for classifiers or recommenders, but adapting it to language models might require checking output diversity across groups defined by sensitive attributes. For example:\\n\\n- Group-based metrics: Evaluate precision, recall, and F1 separately for different demographic subgroups.\\n\\nBut nuance means that bias isn\\'t just present or absent; it could be mitigated based on severity.\\n\\n**Handling Nuance:** Techniques like cosine similarity for fine-grained assessment, uncertainty-aware models, or using pre-trained detectors could help. Also, consider metrics like perplexity but adjust them to account for context and meaning shifts.\\n\\nMaybe I can propose a metric that isn\\'t too simplistic, perhaps one involving multiple steps:\\n\\n- First, evaluate on standard quality metrics (e.g., BERTScore for semantic similarity).\\n\\n- Second, incorporate fairness by measuring bias towards/against certain groups. For instance, compare the probability of positive/negative attributes across different inputs or outputs.\\n\\nBut I need a unified metric.\\n\\n**Ideas for Metrics:**\\n\\n1. **Diversity:** Apply to evaluation data with varied prompts and sensitive attribute variations.\\n\\n2. **Bias Detection from Outputs:** Similar to how fairness metrics work in other contexts. E.g., if an input is gendered, the output should treat everyone equally unless instructed.\\n\\n3. **Training Data Nuance and Bias:** This might be trickier because we need to consider what biases are embedded during training. Perhaps using a test set derived from diverse or biased sources? Or having multiple evaluation datasets.\\n\\nSince I\\'m designing for both, maybe:\\n\\n- Step 1: Assess model outputs for bias against any group, and penalize accordingly.\\n\\nBut how to measure that quantitatively?\\n\\nI can think about metrics like:\\n\\n**GENDER FAIRNESS:** If input doesn\\'t specify gender, but output does with stereotypes, it\\'s biased.\\n\\nThis requires defining sensitive attributes first. Common ones could be \"gender\", \"race\", etc., or general \"fairness\".\\n\\nAnother idea: Use adversarial attack style evaluations to probe biases.\\n\\nBut that might not be practical for a single metric.\\n\\nPerhaps I can draw from the following:\\n\\n- Intrinsic evaluation metrics with bias awareness.\\n\\n- Or, exogenous datasets like Common Crawl (which has training data) and standard benchmarks combined.\\n\\n**Framework:** Maybe frame it as a composite score where:\\n\\n1. There\\'s a baseline quality score based on semantic similarity or other criteria.\\n\\n2. Additional scores measure fairness across different aspects.\\n\\nBut I should aim for one metric if possible.\\n\\nOr, use methods from off-target bias in molecular design? No, that doesn\\'t apply; language is different.\\n\\nIn drug discovery, bias metrics are about specific properties being correlated with the desired effect due to training data artifacts.\\n\\nHere, similarly.\\n\\n**Proposed Name:** Something like \"FaNDiES\": Fairness, Nuance-aware Diversity and Effectiveness Score. But better not come up with an acronym too fancy.\\n\\nKeep it simple: maybe a combination rating (Coherence + Informativeness - Bias) adapted from fairness metrics in AI.\\n\\n**Key Elements for the Metric:**\\n\\n- Quality Dimensions:\\n  - Coherence/Faithfulness\\n  - Fluency/Smoothness\\n  - Relevance\\n\\n- Fairness Factors:\\n  - Reduce stereotypes, harmful content, unfairness, and prejudice.\\n\\nBut nuance means these should be detected sensitively.\\n\\nFor instance, in response quality:\\n\\n- Use a vector database of common sense to measure how aligned responses are with ethical guidelines or diverse perspectives? E.g., BERTScore against a fairness-oriented reference set?\\n\\n**Algorithmic Approach:**\\n\\n1. **Bias Detection Mechanism:** For inputs that might trigger biases (e.g., when discussing genders, using balanced datasets), evaluate the output\\'s tendency and correlate with training data bias.\\n\\nBut I\\'m not providing the implementation code; just designing the metric approach.\\n\\nThink of this as defining an evaluation score to measure fairness or nuance.\\n\\n**Structure the Evaluation Metric:**\\n\\n- Define a set of sensitive attributes (e.g., gender, race).\\n\\n- Use multiple tasks from ethical AI evaluations that test for bias against these attributes. Etc.\\n</think>\\nDesigning a fair and effective language model (LM) evaluation metric is crucial to ensure LMs perform well on meaningful tasks while avoiding the reinforcement or amplification of biases from their training data. The challenge lies in balancing multiple factors: fairness across diverse user groups, capture of nuanced understanding (e.g., context, humor, sarcasm), effectiveness or usefulness for a given purpose, and mitigation of inherent biases.\\n\\nBelow, I\\'ll outline a structured approach to designing such a metric based on established concepts from natural language processing (NLP) and machine learning (ML). Since you\\'re focusing on nuance and bias in both training data and outputs, this requires a composite score that is not just about surface-level metrics but also incorporates depth through semantic similarity or contextual understanding. I\\'ll propose a concept for an evaluation framework called \"NUANS-BiasEvo\" (an acronym for Nuance-Aware, Sensitivity-Informed Evaluation with Bias Metrics and Evolution Tracking), which combines automated assessments from intrinsic benchmarks and human-annotated fairness datasets to be fair throughout the process.\\n\\n### Phase 1: Define Core Components and Objectives\\nThe first step is to set clear goals. Your metric should:\\n1. **Assess Quality:** Focus on coherence, relevance, informativeness, fluency.\\n2. **Account for Nuance:** Capture subtle linguistic elements like context, polysemy, emotion, or cultural shifts (e.g., detecting sarcasm vs. literal interpretation).\\n3. **Handle Fairness and Bias as Features to Maximize/Minimize:**\\n   - From training data (e.g., demographic, historical biases) – reduce unfair patterns and ensure diverse representation.\\n   - From model outputs (e.g., stereotypical responses, unfair content) – quantify bias levels across different sensitive attributes.\\n\\nTo achieve this:\\n- Start with standard LM evaluation metrics as a foundation but modify/hybridize them for fairness. For example:\\n  - **BLEU score** is often used in machine translation to measure token reuse and coherence.\\n  - **ROUGE score** evaluates fluency and n-gram overlap, commonly used in summarization tasks.\\n  - **BERTScore**, based on BERT embeddings from human-written texts (e.g., Wikipedia or CBLUE), measures semantic similarity for general LM response quality.\\n  \\nHowever, these aren\\'t sufficient since they can penalize nuance. For instance, BLEU might not detect if a model is being sarcastic but uses standard rephrasing.\\n\\n### Phase 2: Select Evaluation Methods\\nA fair evaluation must be based on transparent criteria and avoid biases from data sources (e.g., using balanced datasets). Nuance requires more than unigram/word-level models – it needs sentence or discourse-level analysis, while bias detection often involves fine-grained categories like toxicity or stereotyping.\\n\\nI suggest a two-pronged system: one for intrinsic evaluation to provide high-throughput assessment, and another that aligns with human benchmarks but automated to avoid subjectivity.\\n\\n#### 2.1 Intrinsic Evaluation (Baselines)\\nUse the following metrics as the core:\\n- **Semantic similarity and coherence:** BERTScore or related models like GPT embeddings can be used for relevance checks against an ideal reference response.\\n- **Fine-grained nuance detection:** Employ a fine-tuned language model to evaluate context-specific performance, such as sentiment or intent shifts. For example, the \"Discourse Coherence\" score could measure how well responses align with implicit contextual expectations.\\n\\nBut these must be integrated with fairness measures:\\n\\n#### 2.2 Fairness Evaluation\\nFairness metrics typically look at outcomes across groups defined by sensitive attributes (e.g., user identity not directly in input or output). I propose:\\n- **Bias-aware similarity scoring:** Using a reference corpus derived from unbiased data sources, like OSCAR (for common knowledge) or GLUE tasks combined with fairness datasets. For instance, against the BLUE benchmark\\'s stereotype-related references.\\n\\nA key idea here is to base fairness on exogenous factors that aren\\'t tied directly to outputs; for example, by ensuring diverse prompts cover all user roles impartially, then using LM-generated content and evaluating bias independently of quality (e.g., via standard bias tests).\\n\\n#### 2.3 Bias Detection Using Public Data\\n- Leverage publicly available datasets like the Civil Comments dataset or Perspective API from Jigsaw to score outputs for harmful language stereotypes.\\n- For nuance-awareness: Use a vector space model that captures \"fair\" expressions in high-dimensional embeddings, potentially involving domain-specific knowledge bases.\\n\\n### Phase 3: Develop the Composite Metric\\nThe central idea is to use metrics such (Coherence + Informativeness - Harmful Bias) to balance quality and fairness. Here\\'s how:\\n\\n**Formula Base:** Most evaluation scores are additive or harmonic means of submetrics for multi-faceted data.\\n- For instance, BLEURT combines token-overlap with syntax-based measures.\\n\\nFor our metric – let\\'s call it **NUANS-BiasEvo score** – I\\'ll propose a combination:\\n\\\\[ \\\\text{NUANS-BiasEvo Score} = \\\\frac{\\\\alpha}{p + n} (\\\\phi_p + \\\\bar{\\\\phi}) - \\\\lambda b^2 + \\\\mu o \\\\]\\nWhere \\\\( \\\\phi_p \\\\) is the precision of factual output, \\\\( p_n \\\\) refers to nuances like pronouns or emotional contexts that might trigger biases, and bias term handles it.\\n\\nHere are simpler terms for clarity. Start with core components:\\n\\n**Core Quality Metric (Coherence Scale)**:\\n- Evaluate how well outputs maintain context through measures like discourse coherence scores.\\n- Using BERTScore against human-written ground truth references – e.g., responses from reliable sources on a test suite covering various domains and nuance aspects.\\n\\n**Nuance Integration**:\\n- Incorporate the Degree of Ambiguity or Context Adaptation score, using probabilistic models to handle polysemy (e.g., cosine similarity in embedding space for varying interpretations).\\n- Or: Use entity resolution techniques like named-entity linking against Wikipedia to measure context adherence; this can be automated via vector databases.\\n\\n**Bias Metric (Fairness Component)**:\\n- Start with a \"Sensitivity\" score from biased outputs – e.g., how often the model includes stereotypical or unfair content when not instructed.\\n  - This is measured by applying fairness filters, such as comparing to balanced prompts and references.\\n\\n### Phase 4: Handling Nuance and Bias in Unified Way\\nNuance often requires context-sensitive embedding methods. Intrinsic metrics can cover this if they use appropriate datasets – e.g., a test set with diverse user perspectives or roles (gender, profession, etc.).\\n\\n**Bias Detection from Training Data**: This is not directly measurable through outputs alone; it would require examining training data artifacts and how LM handles them during prompting.\\n- For instance: By ensuring the test environment includes samples from various sources.\\n\\n#### 4.1 Metric Normalization\\n- Normalize scores based on fairness to avoid unfair performance differences, similar to off-target bias handling in bioinformatics but adapted for language output quality metrics.\\n\\n### Phase 4: Practical Implementation Considerations\\n- **Sources of Nuance and Bias**: Use a test suite including adversarial examples (e.g., from Fairseq\\'s Hugging Face model evaluation) or real-time interaction logs for unbiased data mining.\\n- **Scoring System:**\\n  - Normalize the metric by dividing by group-specific scores to average performance across subgroups, e.g., F1 per subgroup and weight appropriately to handle small datasets.\\n\\n### Phase 5: Testing and Validation\\nTo ensure fairness and effectiveness:\\n- Test on multiple standard benchmarks: BLEU for language tasks combined with human evaluations from diverse demographic groups (from existing ethical AI programs).\\n- Measure bias against user groups by inputting similar prompts without gender indicators, but checking outputs – e.g., if given a query requiring generalization across genders.\\n\\n### Example Calculation\\nImagine an LM being evaluated on answering customer service queries for fairness in response.\\n1. **Baseline Score:** BERTScore or another semantic similarity metric (e.g., P3 accuracy from evaluations).\\n2. **Bias Scores:** Using Google\\'s Perspective API to score outputs against a threshold of harmfulness, then adjust the base score by subtracting points proportionally.\\n\\nIf I use Python for pseudocode representation:\\n\\n```python\\nimport transformers  # from HuggingFace for BERTScore\\n\\nclass NUANSESBiasEvaluation:\\n    def __init__(self):\\n        self.model = transformers.pipeline(\"text-generation\", model=\"gpt2-xl\")\\n        self.bert_model = transformers.TFBertForQuestionAnswering.from_pretrained(\\'bert-base-uncased\\')\\n\\n   # Function to evaluate coherence\\n   def evaluate_coherence(response, reference):\\n        return 1 - (cosine_sim of embeddings between response and reference)\\n\\n   # Function to assess nuance – e.g., context-specific understanding\\n   def evaluate_nuance(prompt, responses):\\n        embeddings = get_context_embeddings()  # Using high-level representation from NTP or something similar.\\n        scores = [ cosine similarity of embedded prompt vs. embedded response ]\\n\\n   # Fairness component: Check for bias by inputting prompts with varying sensitive attributes (e.g., gender-neutral alternatives) and scoring outputs on stereotype content.\\n\\ndef combined_score(metric, base_scores):\\n    normalized_score = 0\\n    all_demographics = {\"woman\", \"man\", \"LGBTQ+\", etc.} # Use a pre-defined list of common biases based on existing research.\\n    for demo in demographics:\\n        sub_metric = measure_bias_at_this_level(base_score_for_this_demo) \\n        score_adjust = apply_normalization()\\n        normalization_constant\\n```\\nTo improve nuance handling, perhaps use fine-tuned LMs like ESEMELICA or GRL-46-BASELINE from existing research to handle specific cases better.\\n\\n### Final Recommendation\\nStart with an automated approach based on intrinsic evaluation metrics and expand using test sets that capture fairness dimensions automatically. If you have access to tools for bias detection, they can be integrated into a single composite score. This way, it\\'s fair (by cross-referencing groups) effective overall while accounting for nuance through advanced textual representations – rather than penalizing model behaviors like \"off-target\" bias in training data.\\n\\nYou might also consider building on existing frameworks to keep development simpler.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1755015854, model='deepseek-r1:latest', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=3433, prompt_tokens=33, total_tokens=3466, completion_tokens_details=None, prompt_tokens_details=None))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef02aa05",
   "metadata": {},
   "source": [
    "### LM Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bda1ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_studio = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lmstudio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68e6fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = lm_studio.chat.completions.create(\n",
    "    model=\"model-identifier\",\n",
    "    messages=[{\"role\": \"user\", \"content\": question}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "266211eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1. Goal & Scope\n",
      "\n",
      "| Aspect | What we want to measure |\n",
      "|--------|------------------------|\n",
      "| **Language quality** | Fluency, coherence, relevance, and informativeness. |\n",
      "| **Bias & Fairness** | Whether the model reproduces or amplifies harmful stereotypes, discriminatory language, or socially sensitive content. |\n",
      "| **Nuance** | Ability to handle subtle distinctions (tone, politeness, sarcasm, domain‑specific jargon). |\n",
      "\n",
      "The evaluation metric should be *composite*: a single score that blends quality, bias‑penalty, and nuance‑reward, but also provides decomposed diagnostics so practitioners can understand *why* a model scored low.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Data Foundations\n",
      "\n",
      "### 2.1 Curated Test Suites\n",
      "- **Multi‑genre prompts** (news, fiction, dialogue, code) to test generality.\n",
      "- **Bias‑aware prompts**: \n",
      "  - *Demographic probes*: “What do women/men/mixed‑race people do in…?”\n",
      "  - *Sensitive topic prompts*: mental health, politics, religion.\n",
      "  - *Stereotype tests*: “Describe a typical engineer…” vs. “Describe a typical nurse…”\n",
      "- **Nuance probes**:\n",
      "  - *Tone shifts*: “I’m upset” → request calm; “I’m excited” → enthusiastic response.\n",
      "  - *Sarcasm & irony detection*: “Oh, great job!”\n",
      "  - *Domain‑specific jargon* (medical, legal, tech).\n",
      "\n",
      "### 2.2 Ground Truth Construction\n",
      "- **Human annotation**: crowdsource diverse annotators for each prompt to rate:\n",
      "  - Fluency / grammaticality\n",
      "  - Relevance / correctness\n",
      "  - Bias presence & severity\n",
      "  - Tone alignment / nuance capture\n",
      "- Use *multiple* annotators per sample; compute inter‑annotator agreement (Krippendorff’s alpha) and calibrate scores.\n",
      "\n",
      "### 2.3 Reference Distribution\n",
      "- Build a **bias‑distribution baseline** from the training data:\n",
      "  - Compute term frequency–inverse document frequency (TF‑IDF) for demographic / sensitive terms.\n",
      "  - Measure co‑occurrence statistics of protected attributes with job roles, traits, etc.\n",
      "  - This serves as a “worst‑case” reference; model outputs are penalized if they exceed the training bias distribution.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Core Metric Components\n",
      "\n",
      "| Component | Formula (simplified) | Rationale |\n",
      "|-----------|----------------------|-----------|\n",
      "| **Quality Score (Q)** | `Q = β1 * Fluency + β2 * Relevance + β3 * Informativeness` | Standard NLP metrics (BLEU, ROUGE, BERTScore, Perplexity) weighted by human‑rated importance. |\n",
      "| **Bias Penalty (B)** | `B = Σ_i w_i * max(0, P_out(i) – P_train(i))` where `i` indexes protected attribute terms | Penalizes over‑representation of biased language relative to training distribution. |\n",
      "| **Nuance Reward (N)** | `N = γ1 * ToneMatch + γ2 * SarcasmDetectionAccuracy + γ3 * JargonCoverage` | Rewards correct handling of subtle linguistic phenomena. |\n",
      "| **Composite Score (C)** | `C = Q – B + N` | Balances quality with fairness, then enhances nuance. |\n",
      "\n",
      "- **Weights (`β`, `w`, `γ`)** are tuned via validation on a held‑out set of human scores.\n",
      "- Each component can be normalized to `[0,1]` before combination.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Bias Modeling Details\n",
      "\n",
      "### 4.1 Protected Attribute Identification\n",
      "- Use an ontology (e.g., Gender, Race, Age, Disability) mapped to lexical cues and embeddings.\n",
      "- Employ *contextualized* bias detection: extract the probability that a token is associated with a protected attribute in the model’s hidden states.\n",
      "\n",
      "### 4.2 Distribution Comparison\n",
      "- **Kolmogorov–Smirnov (KS)** test or Earth‑Mover Distance (EMD) between output and training distributions for each attribute.\n",
      "- Convert distance into penalty: `B_i = α * D_i` where `α` scales the influence of that attribute.\n",
      "\n",
      "### 4.3 Counterfactual Fairness Check\n",
      "- For a subset of prompts, generate counterfactual outputs by swapping protected attributes (e.g., “she” ↔ “he”) and measure difference in sentiment or content. Large disparities indicate bias.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Nuance Detection Techniques\n",
      "\n",
      "| Nuance | Detection Method |\n",
      "|--------|------------------|\n",
      "| **Tone** | Use a pre‑trained sentiment + politeness classifier; compute similarity between predicted tone vector and target tone vector. |\n",
      "| **Sarcasm/Irony** | Fine‑tune a sarcasm detector on labeled corpora; reward correct identification of sarcastic intent. |\n",
      "| **Jargon / Domain Knowledge** | Build a dictionary of domain terms; compute coverage ratio. Alternatively, use an embedding similarity to a set of known domain concepts. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Evaluation Workflow\n",
      "\n",
      "1. **Generate outputs** for all prompts using the target model.\n",
      "2. **Compute Q, B, N** per prompt:\n",
      "   - Use automatic metrics (BLEU/ROUGE) + human‑derived labels where available.\n",
      "   - Run bias detectors and nuance classifiers on each output.\n",
      "3. **Aggregate**:\n",
      "   - Average `Q`, `B`, `N` across prompts within a *category* (e.g., gender, tone).\n",
      "   - Compute overall `C`.\n",
      "4. **Interpretation Dashboard**:\n",
      "   - Show per‑attribute bias heatmap.\n",
      "   - Highlight top‑scoring and bottom‑scoring samples.\n",
      "   - Provide actionable insights (e.g., “Model over‑represents ‘engineer’ with male pronouns in 37% of cases”).\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Validation & Calibration\n",
      "\n",
      "- **Human‑in‑the‑loop validation**: Randomly sample outputs, have experts rate them against the composite score to verify alignment.\n",
      "- **Calibration curves**: Plot predicted bias penalty vs. human bias ratings; adjust `w_i` weights until slope ≈ 1.\n",
      "- **Robustness tests**:\n",
      "  - *Adversarial prompts*: e.g., “Describe a typical engineer” with hidden protected attributes.\n",
      "  - *Distribution shift*: evaluate on out‑of‑distribution data (e.g., different dialects).\n",
      "\n",
      "---\n",
      "\n",
      "## 8. Deployment & Continuous Monitoring\n",
      "\n",
      "| Stage | Action |\n",
      "|-------|--------|\n",
      "| **Baseline** | Measure the composite score of current model. |\n",
      "| **Model updates** | After each fine‑tuning or architectural change, re‑evaluate; trigger alerts if `C` drops below a threshold. |\n",
      "| **Dataset drift detection** | Monitor changes in training data bias distribution; retrain when significant shifts occur. |\n",
      "| **User feedback loop** | Allow end‑users to flag biased outputs; feed these back into the evaluation pipeline as real‑world samples. |\n",
      "\n",
      "---\n",
      "\n",
      "## 9. Example Calculation\n",
      "\n",
      "Assume a single prompt:\n",
      "\n",
      "- Human scores: Fluency = 0.9, Relevance = 0.8, Informativeness = 0.85.\n",
      "- Model output shows bias term “engineer” with male pronoun; training distribution for this pairing is 0.05, model frequency is 0.12 → `D = 0.07`.\n",
      "- Tone correctly matched: 1.0.\n",
      "- Sarcasm detected correctly: 1.0.\n",
      "- Jargon coverage 0.9.\n",
      "\n",
      "Weights: β1=β2=β3=0.33; w=5 (penalty scaling); γ1=γ2=γ3=0.25.\n",
      "\n",
      "Compute:\n",
      "\n",
      "`Q = 0.33*(0.9+0.8+0.85) ≈ 0.83`\n",
      "\n",
      "`B = 5 * max(0, 0.07) = 0.35`\n",
      "\n",
      "`N = 0.25*(1.0 + 1.0 + 0.9) = 0.725`\n",
      "\n",
      "`C = 0.83 - 0.35 + 0.725 ≈ 1.205` (normalized to [0,1] if needed).\n",
      "\n",
      "A high `B` reduces the score; a strong nuance reward can offset moderate bias but not severe bias.\n",
      "\n",
      "---\n",
      "\n",
      "## 10. Final Thoughts\n",
      "\n",
      "- **Transparency**: Publish all prompt sets, annotations, and code for metric computation.\n",
      "- **Community Feedback**: Encourage researchers to suggest new probes or adjust weights to reflect evolving fairness norms.\n",
      "- **Iterative Refinement**: Treat the composite metric as a living document; update it when new bias phenomena emerge or better nuance detectors are released.\n",
      "\n",
      "By blending human judgment, statistical bias checks, and nuanced language detection into a single, tunable score—and by providing fine‑grained diagnostics—this approach offers both *fairness* and *effectiveness* in evaluating modern language models.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ddfbde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
